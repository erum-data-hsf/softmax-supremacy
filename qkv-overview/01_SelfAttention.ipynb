{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74795b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8f68367",
   "metadata": {},
   "source": [
    "\n",
    "**Introduction to QKV Self-Attention Mechanism in Encoders: The Detective Analogy**  \n",
    "\n",
    "Imagine a detective solving a complex case with scattered clues. To understand the story, the detective systematically evaluates each clue's relevance and connection, similar to the Query-Key-Value (QKV) self-attention mechanism in a transformer encoder.\n",
    "\n",
    "In this analogy:\n",
    "\n",
    "* Queries (Q) are the detective's questions or focal points.\n",
    "* Keys (K) represent the unique features of each clue.\n",
    "* Values (V) are the actual pieces of evidence.\n",
    "\n",
    "\n",
    "The self-attention mechanism works in three steps:\n",
    "* Scoring Relevance: The detective compares queries to keys to score each clue's relevance.\n",
    "* Normalizing Scores: These scores are normalized using a softmax function, akin to the detective assigning a probability to each clue.\n",
    "* Aggregating Information: The detective gathers information from the most relevant clues, weighted by their scores, to piece together the story. \n",
    "\n",
    "\n",
    "This mechanism helps the encoder capture relationships within the input data, just as a detective uncovers connections in a case, making it essential for modern natural language processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07fe18c3",
   "metadata": {},
   "source": [
    "# Attention Mechanisms\n",
    "This exercise is based on Lukas Heinrich's lecture about Transformers in the course \"Modern Deep Learning in Physics\" @ TUM. \n",
    "In this exercise we will write a attention mechanism to solve the following problem\n",
    "\n",
    "\"Given a length 10 sequence of integers, are there more '2s' than '4s' ?\"\n",
    "\n",
    "This could of course be easily handled by a fully connected network, but we'll force the network\n",
    "to learn this by learning to place attention on the right values. I.e. the strategy is\n",
    "\n",
    "* embed the individual integer into a high-dimensional vector (using torch.nn.Embedding)\n",
    "* once we have those embeddings compute how much attention to place on each vector by comparing \"key\" values computed from the embedded values \n",
    "* compute \"answer\" values to our query by weighting the individual responsed by their attention value\n",
    "\n",
    "\n",
    "$$\n",
    "v_{ik} = \\mathrm{softmax}_\\mathrm{keys}(\\frac{q_{i}\\cdot k_{j}}{\\sqrt{d}}) v_{jk}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c475d8",
   "metadata": {},
   "source": [
    "## Preparation\n",
    "Before we bring this all to live in a written class, we go through each step in order to understand it properly. \n",
    "\n",
    "We will use the QKV - Self Attention Encoding Part.\n",
    "For doing so we will replicate the shown procedure:\n",
    "\n",
    "\n",
    "<img src=\"Grafiken/text1.svg\" alt=\"Attention\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7ac09c",
   "metadata": {},
   "source": [
    "### Generating Data\n",
    "Write a data-generating function that produces a batch of N examples or length-10 sequences of random integers between 0 and 9 as well as a binary label indicating whether the sequence has more 2s than 4s. The output should return (X,y), where X has shape `(N,10)` and y has shape `(N,1)`\n",
    "\n",
    "You can use torch.randint() for creating the batch.\n",
    "\n",
    "```python\n",
    "def make_batch(N):\n",
    "    ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9edb606f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "78b024d4",
   "metadata": {},
   "source": [
    "### Embedding the Integers\n",
    "Deep Learning works well in higher dimensions. So we'll embed the 10 possible integers into a vector space using `torch.nn.Embedding`\n",
    "\n",
    "\n",
    "* Verify that using e.g. a module like `torch.nn.Embedding(10,embed_dim)` achieves this\n",
    "* Take a random vector of integers of shape (N,M) and evaluate them through an embedding module\n",
    "* Does the output dimension make sense?\n",
    "\n",
    "**Alternatively:**\n",
    "\n",
    "\n",
    "One can embed the integers inot a vector space such that one uses One-Hot Encoding and sending it through a `torch.nn.Linear(one_hot_dim, embed_dim)`\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b27efe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b03c2bd9",
   "metadata": {},
   "source": [
    "### Extracting Keys and Values \n",
    "\n",
    "Once data is embedded we can extract keys and values by a linear projection\n",
    "\n",
    "* For 2 linear layers `torch.nn.Linear(embed_dim,att_dim)` we can extract keys and values for the output of the previous step\n",
    "* verify that this works from a shape perspective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c810e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8c9e8ab8",
   "metadata": {},
   "source": [
    "## Computing Attention\n",
    "<img src=\"Grafiken/text1.svg\" alt=\"Attention\">\n",
    "Implement the Attention-formula from above in a batched manner, such that for a input set of sequences `(N,10)`\n",
    "you get an output set of attention-weighted values `(N,1)`\n",
    "\n",
    "* It's easiest when using the function `torch.einsum` which uses the Einstein summation you may be familiar with from special relativity\n",
    "* e.g. a \"batched\" dot product is performed using `einsum('bik,bjk->bij')` where `b` indicates the batch index, `i` and `j` are position indices and `k` are the coordinates of the vectors\n",
    "\n",
    "\n",
    "--> **Some hints**\n",
    "* Keep in mind that the dimension $\\sqrt{d}$ in the softmax function is your `att_dim`\n",
    "* Initiate your query randomly in the size `1,att_dim`\n",
    "* query and keys: einsum --> `'ik,bjk->bij'`\n",
    "* for the softmax use `dim=-1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d84e5a52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "456c36f7",
   "metadata": {},
   "source": [
    "# Integrate into a Module\n",
    "\n",
    "Complete the following torch Module:\n",
    "\n",
    "To use the `self.nn` make sure to have an input shaped like `torch.Size([batch, att_dim])`\n",
    "\n",
    "\n",
    "* For the `forward(x)` function have a look at the Graph in 1.2 again and follow along\n",
    "```python\n",
    "class AttentionModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AttentionModel,self).__init__()\n",
    "        self.embed_dim = 5\n",
    "        self.att_dim = 5\n",
    "        self.embed = torch.nn.Embedding(10,self.embed_dim)\n",
    "        \n",
    "        #one query\n",
    "        self.query  = torch.nn.Parameter(torch.randn(1,self.att_dim))\n",
    "        \n",
    "        #used to compute keys\n",
    "        self.WK = torch.nn.Linear(self.embed_dim,self.att_dim)\n",
    "        \n",
    "        #used to compute values\n",
    "        self.WV = torch.nn.Linear(self.embed_dim,1)\n",
    "        \n",
    "        #final decision based on attention-weighted value\n",
    "        self.nn = torch.nn.Sequential(\n",
    "            torch.nn.Linear(1,200),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(200,1),\n",
    "            torch.nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def attention(self,x):\n",
    "        # compute attention\n",
    "        ...\n",
    "    \n",
    "    def values(self,x):\n",
    "        # compute values\n",
    "        ...\n",
    "                \n",
    "    def forward(self,x):\n",
    "        # compute final classification using attention, values and final NN\n",
    "      \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4fbb672",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "966c8496",
   "metadata": {},
   "source": [
    "## Predefine Plot Function\n",
    "Use this given function later on to visualize your training. \n",
    "Just execute the following cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66639c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(model,N,traj):\n",
    "    x,y = make_batch(N)\n",
    "    f,axarr = plt.subplots(1,3)\n",
    "    f.set_size_inches(10,2)\n",
    "    ax = axarr[0]\n",
    "    at = model.attention(model.embed(x))[:,0,:].detach().numpy()\n",
    "    ax.imshow(at)\n",
    "    ax = axarr[1]\n",
    "    \n",
    "    \n",
    "    vals = model.values(model.embed(x))[:,:,0].detach().numpy()\n",
    "    nan = np.ones_like(vals)*np.nan\n",
    "    nan = np.where(at > 0.1, vals, nan)\n",
    "    ax.imshow(nan,vmin = -1, vmax = 1)\n",
    "    for i,xx in enumerate(x):\n",
    "        for j,xxx in enumerate(xx):\n",
    "            ax = axarr[0]\n",
    "            ax.text(j,i,xxx.numpy(), c = 'r' if (xxx in [2,4]) else 'w')    \n",
    "            ax = axarr[1]\n",
    "            ax.text(j,i,xxx.numpy(), c = 'r' if (xxx in [2,4]) else 'w')    \n",
    "    ax = axarr[2]\n",
    "    ax.plot(traj)\n",
    "    f.set_tight_layout(True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef15d048",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "45f3aaa4",
   "metadata": {},
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972ecdf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    model = AttentionModel()\n",
    "    opt = torch.optim.Adam(model.parameters(),lr = 1e-4)\n",
    "\n",
    "    traj = []\n",
    "    for i in range(5001):\n",
    "        x,y = make_batch(100)\n",
    "        p = model.forward(x)\n",
    "        loss = torch.nn.functional.binary_cross_entropy(p,y)\n",
    "        loss.backward()\n",
    "        traj.append(float(loss))\n",
    "        if i % 500 == 0:\n",
    "            plot(model,5,traj)\n",
    "            # plt.savefig('attention_{}.png'.format(str(i).zfill(6)))\n",
    "            plt.show()\n",
    "            print(i,loss)\n",
    "        opt.step()\n",
    "        opt.zero_grad()\n",
    "    return traj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dee3bb6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "training = train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
